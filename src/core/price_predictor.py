import numpy as np
import pandas as pd
import requests
import feedparser
from textblob import TextBlob
from datetime import datetime, timedelta
import urllib.parse
import time
import random
import re
import json
import traceback
import config

# å°è¯•å¯¼å…¥æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.preprocessing import MinMaxScaler
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Input
    import os

    # æŠ‘åˆ¶ TensorFlow çš„çƒ¦äººæ—¥å¿—
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    HAS_ML = True
except ImportError as e:
    HAS_ML = False
    print(f"âš ï¸ æœªæ£€æµ‹åˆ° TensorFlow/Sklearnï¼Œé¢„æµ‹æ¨¡å—å°†è¿è¡Œåœ¨ç®€æ˜“æ¨¡å¼ã€‚é”™è¯¯: {e}")


class NameTranslator:
    """ä¸­æ–‡é¥°å“åè½¬è‹±æ–‡ Market Hash Name"""

    def __init__(self):
        self.cn_to_en_items = {}
        self.suggestion_list = []
        self.condition_map = {
            "å´­æ–°å‡ºå‚": "Factory New", "ç•¥æœ‰ç£¨æŸ": "Minimal Wear",
            "ä¹…ç»æ²™åœº": "Field-Tested", "ç ´æŸä¸å ª": "Well-Worn",
            "æˆ˜ç—•ç´¯ç´¯": "Battle-Scarred", "å´­æ–°": "Factory New",
            "ç•¥ç£¨": "Minimal Wear", "ä¹…ç»": "Field-Tested",
            "ç ´æŸ": "Well-Worn", "æˆ˜ç—•": "Battle-Scarred"
        }
        self.en_cond_to_cn = {
            "Factory New": "å´­æ–°å‡ºå‚", "Minimal Wear": "ç•¥æœ‰ç£¨æŸ",
            "Field-Tested": "ä¹…ç»æ²™åœº", "Well-Worn": "ç ´æŸä¸å ª",
            "Battle-Scarred": "æˆ˜ç—•ç´¯ç´¯"
        }
        self._load_db()

    def _load_db(self):
        try:
            with open(config.DB_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                standard_conds = ["Factory New", "Minimal Wear", "Field-Tested", "Well-Worn", "Battle-Scarred"]

                for col, tiers in data.items():
                    for tier, items in tiers.items():
                        for item in items:
                            name_en = item.get('name')
                            name_cn = item.get('name_cn')

                            if name_en:
                                if name_cn: self.cn_to_en_items[name_cn] = name_en
                                self.cn_to_en_items[name_en] = name_en

                                for cond in standard_conds:
                                    full_en = f"{name_en} ({cond})"
                                    self.suggestion_list.append(full_en)
                                    if name_cn:
                                        cn_cond = self.en_cond_to_cn.get(cond, "")
                                        full_cn = f"{name_cn} ({cn_cond})"
                                        self.suggestion_list.append(full_cn)
                                self.suggestion_list.append(name_en)
                                if name_cn: self.suggestion_list.append(name_cn)

        except Exception as e:
            print(f"âŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")

    def get_all_names(self):
        return sorted(list(set(self.suggestion_list)))

    def translate(self, user_input):
        user_input = user_input.strip()
        target_cond = ""
        clean_input = user_input

        for cn, en in self.condition_map.items():
            if cn in user_input:
                target_cond = en
                clean_input = clean_input.replace(cn, "").replace("()", "").replace("ï¼ˆï¼‰", "").strip()
                break

        if not target_cond:
            ens = ["Factory New", "Minimal Wear", "Field-Tested", "Well-Worn", "Battle-Scarred"]
            for en in ens:
                if en.lower() in user_input.lower():
                    target_cond = en
                    clean_input = re.sub(re.escape(en), "", clean_input, flags=re.IGNORECASE).replace("()", "").replace(
                        "ï¼ˆï¼‰", "").strip()
                    break

        clean_input = clean_input.strip(" |")

        real_name = self.cn_to_en_items.get(clean_input)
        if not real_name:
            for cn, en in self.cn_to_en_items.items():
                if clean_input in cn or clean_input.lower() in en.lower():
                    real_name = en
                    break

        if not real_name: real_name = clean_input

        if target_cond:
            return f"{real_name} ({target_cond})"
        return real_name


class DataFetcher:
    def __init__(self, cookie=None):
        self.cookie = cookie
        self.base_url = "https://steamcommunity.com/market/pricehistory/"
        self.translator = NameTranslator()

        # âœ… ä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼šæ‰‹åŠ¨æœˆä»½æ˜ å°„
        # å³ä½¿ç³»ç»Ÿæ˜¯ä¸­æ–‡ï¼Œä¹Ÿèƒ½æ­£ç¡®è§£æ Steam çš„è‹±æ–‡æœˆä»½
        self.month_map = {
            "Jan": 1, "Feb": 2, "Mar": 3, "Apr": 4, "May": 5, "Jun": 6,
            "Jul": 7, "Aug": 8, "Sep": 9, "Oct": 10, "Nov": 11, "Dec": 12
        }

    def fetch_price_history(self, user_input_name):
        market_hash_name = self.translator.translate(user_input_name)
        print(f"ğŸ” è§£æé¥°å“å: {market_hash_name}")

        encoded_name = urllib.parse.quote(market_hash_name)
        # currency=23 æ˜¯äººæ°‘å¸
        url = f"{self.base_url}?country=CN&currency=23&appid=730&market_hash_name={encoded_name}"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://steamcommunity.com/market/",
        }

        if self.cookie:
            headers["Cookie"] = f"steamLoginSecure={self.cookie}"
            print(f"âœ… ä½¿ç”¨ Cookie (é•¿åº¦: {len(self.cookie)})")
        else:
            print("âš ï¸ æœªæä¾› Cookieï¼Œå°è¯•åŒ¿åè·å–ï¼ˆå¯èƒ½å¤±è´¥ï¼‰")

        try:
            print(f"ğŸ“¡ è¯·æ±‚ Steam API...")
            response = requests.get(url, headers=headers, timeout=15)

            if response.status_code == 200:
                data = response.json()

                if data and 'prices' in data:
                    raw_count = len(data['prices'])
                    print(f"âœ… è·å–åˆ° {raw_count} æ¡åŸå§‹ä»·æ ¼æ•°æ®")
                    if raw_count == 0:
                        return None, "Steam è¿”å›äº†ç©ºæ•°æ® (å¯èƒ½ç‰©å“æš‚æ— æˆäº¤)"
                    return self._process_raw_data(data['prices'])
                else:
                    return None, "API å“åº”æ ¼å¼é”™è¯¯ (æœªæ‰¾åˆ° prices å­—æ®µ)"

            elif response.status_code == 429:
                return None, "è¯·æ±‚è¿‡äºé¢‘ç¹ (HTTP 429)"
            elif response.status_code == 403:
                return None, "æ— æƒè®¿é—® (HTTP 403) - Cookie å¯èƒ½å·²å¤±æ•ˆæˆ–éœ€è¦ç™»å½•"
            else:
                return None, f"è¯·æ±‚å¤±è´¥ HTTP {response.status_code}"

        except requests.exceptions.Timeout:
            return None, "è¯·æ±‚ Steam è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œä»£ç†"
        except Exception as e:
            print(f"âŒ å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            return None, f"è¯·æ±‚å¼‚å¸¸: {str(e)}"

    def _process_raw_data(self, raw_data):
        clean = []
        parse_errors = 0
        success_count = 0

        # Steam æ ¼å¼ç¤ºä¾‹: ["Nov 14 2023 01: +0", 1.23, "100"]
        for p in raw_data:
            try:
                date_part = p[0].split(":")[0]  # "Nov 14 2023 01"
                # æ‰‹åŠ¨è§£æï¼Œä¸ä½¿ç”¨ strptime çš„ %bï¼Œé¿å… locale é—®é¢˜
                parts = date_part.split()  # ['Nov', '14', '2023', '01']
                if len(parts) >= 3:
                    month_str = parts[0]
                    day = int(parts[1])
                    year = int(parts[2])

                    month = self.month_map.get(month_str, 1)
                    dt = datetime(year, month, day)

                    clean.append({"Date": dt, "Price": float(p[1]), "Volume": int(p[2])})
                    success_count += 1
                else:
                    parse_errors += 1
            except Exception:
                parse_errors += 1

        if parse_errors > 0:
            print(f"âš ï¸ è§£æå¤±è´¥ {parse_errors} æ¡æ•°æ® (æˆåŠŸ {success_count} æ¡)")

        df = pd.DataFrame(clean)
        if df.empty:
            print("âŒ æ•°æ®è§£æåä¸ºç©º")
            return None, "æ•°æ®è§£æå¤±è´¥ (æ—¥æœŸæ ¼å¼ä¸å…¼å®¹)"

        # æŒ‰æ—¥èšåˆ
        df_daily = df.groupby('Date').agg({'Price': 'mean', 'Volume': 'sum'}).reset_index()

        # è¿‡æ»¤æœ€è¿‘ 365 å¤©
        today = datetime.now()
        one_year_ago = today - timedelta(days=365)
        df_daily = df_daily[(df_daily['Date'] >= one_year_ago) & (df_daily['Date'] <= today)]

        final_count = len(df_daily)
        print(f"âœ… æœ€ç»ˆæœ‰æ•ˆå†å²æ•°æ®: {final_count} å¤©")

        if final_count < 14:
            return None, f"è¿‘æœŸæ•°æ®ä¸è¶³ (ä»… {final_count} å¤©)ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆé¢„æµ‹"

        return df_daily.sort_values('Date'), "Success"


class SentimentAnalyzer:
    def __init__(self):
        self.sources = [
            "https://news.google.com/rss/search?q=CS2+Counter-Strike+Skins+Market&hl=en-US&gl=US&ceid=US:en",
            "https://www.reddit.com/r/csgomarketforum/new/.rss"
        ]

    def get_market_sentiment(self):
        total_p = 0
        count = 0
        for url in self.sources:
            try:
                feed = feedparser.parse(url)
                if not feed.entries: continue
                for entry in feed.entries[:5]:
                    blob = TextBlob(entry.title)
                    total_p += blob.sentiment.polarity
                    count += 1
            except Exception:
                pass

        if count == 0: return 0.0, "ä¸­æ€§ (æ— æ•°æ®)"
        score = np.tanh((total_p / count) * 5)

        if score > 0.25:
            status = "è´ªå©ª (Greedy) ğŸ‚"
        elif score < -0.25:
            status = "ææ…Œ (Fear) ğŸ»"
        else:
            status = "ä¸­æ€§ (Neutral) âš–ï¸"
        return score, status


class PricePredictor:
    def __init__(self, df):
        self.df = df
        self.look_back = 15  # è§‚å¯Ÿçª—å£
        self.forecast_days = 7

    def predict(self):
        if self.df is None or len(self.df) < 30:
            return None, "æ•°æ®é‡ä¸è¶³ä»¥è¿›è¡Œé¢„æµ‹"

        if HAS_ML:
            try:
                return self._predict_lstm()
            except Exception as e:
                print("âš ï¸ LSTM é¢„æµ‹å‘ç”Ÿä¸¥é‡é”™è¯¯:")
                traceback.print_exc()
                print("ğŸ‘‰ è‡ªåŠ¨é™çº§ä¸ºçº¿æ€§é¢„æµ‹")
                return self._predict_linear()
        else:
            print("â„¹ï¸ æœªå®‰è£… AI åº“ï¼Œä½¿ç”¨çº¿æ€§é¢„æµ‹")
            return self._predict_linear()

    def _predict_lstm(self):
        # æ•°æ®é¢„å¤„ç†
        data = self.df['Price'].values.reshape(-1, 1)
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled = scaler.fit_transform(data)

        X, Y = [], []
        for i in range(len(scaled) - self.look_back):
            X.append(scaled[i:i + self.look_back, 0])
            Y.append(scaled[i + self.look_back, 0])

        if len(X) == 0:
            raise ValueError("æ•°æ®ä¸è¶³ä»¥æ„å»º LSTM åºåˆ—")

        X = np.reshape(np.array(X), (len(X), self.look_back, 1))
        Y = np.array(Y)

        # æ„å»ºæ¨¡å‹
        model = Sequential()
        model.add(Input(shape=(self.look_back, 1)))
        # å¢åŠ ç¥ç»å…ƒå’Œå±‚æ•°ï¼Œæé«˜æ‹Ÿåˆèƒ½åŠ›
        model.add(LSTM(64, return_sequences=True))
        model.add(LSTM(32, return_sequences=False))
        model.add(Dense(16, activation='relu'))
        model.add(Dense(1))

        # å¢åŠ  epoch æ•°ï¼Œé¿å…æ¬ æ‹Ÿåˆå¯¼è‡´ç›´çº¿
        model.compile(loss='mse', optimizer='adam')
        model.fit(X, Y, epochs=30, batch_size=8, verbose=0)

        # é€’å½’é¢„æµ‹
        preds = []
        curr = scaled[-self.look_back:].reshape(1, self.look_back, 1)

        for _ in range(self.forecast_days):
            p = model.predict(curr, verbose=0)[0]
            preds.append(p)
            # æ»‘åŠ¨çª—å£
            curr = np.append(curr[:, 1:, :], [[p]], axis=1)

        final_prices = scaler.inverse_transform(np.array(preds).reshape(-1, 1)).flatten()
        dates = [self.df['Date'].iloc[-1] + timedelta(days=i) for i in range(1, self.forecast_days + 1)]

        return pd.DataFrame({'Date': dates, 'Price': final_prices}), None

    def _predict_linear(self):
        """
        æ”¹è¿›ç‰ˆçº¿æ€§å›å½’
        ä¸å†ç”»æ­»æ¿çš„ç›´çº¿ï¼Œè€Œæ˜¯åŸºäºå†å²æ³¢åŠ¨ç‡ç”Ÿæˆéšæœºæ¸¸èµ° (Random Walk with Drift)
        """
        last_p = self.df['Price'].iloc[-1]
        recent = self.df['Price'].iloc[-20:]  # çœ‹æœ€è¿‘20å¤©

        # è®¡ç®—ç®€å•è¶‹åŠ¿
        x = np.arange(len(recent))
        y = recent.values
        z = np.polyfit(x, y, 1)  # 1æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
        trend = z[0]  # æ–œç‡

        # è®¡ç®—å†å²æ³¢åŠ¨ç‡ (æ ‡å‡†å·®)
        std_dev = recent.std()

        dates = []
        prices = []

        current_p = last_p
        for i in range(1, self.forecast_days + 1):
            # è¶‹åŠ¿ + éšæœºæ‰°åŠ¨ (æ¨¡æ‹Ÿå¸‚åœºå™ªéŸ³)
            # ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒï¼Œæ ‡å‡†å·®å–å†å²çš„ä¸€åŠï¼Œé¿å…æ³¢åŠ¨è¿‡å¤§
            noise = random.gauss(0, std_dev * 0.6)
            current_p += trend + noise

            dates.append(self.df['Date'].iloc[-1] + timedelta(days=i))
            prices.append(max(0.01, current_p))  # ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿ

        return pd.DataFrame({'Date': dates, 'Price': prices}), "Warning: Running in Linear Mode (Low Accuracy)"